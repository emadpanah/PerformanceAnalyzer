import cv2  # Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙˆÛŒØ¯Ø¦Ùˆ Ùˆ ØªØµØ§ÙˆÛŒØ±
import mediapipe as mp  # Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø­Ø§Ù„Øª Ø¨Ø¯Ù†
import numpy as np  # Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø¹Ø¯Ø¯ÛŒ
import pandas as pd  # Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
import streamlit as st  # Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯
import plotly.express as px  # Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§
import argparse  # Ø¨Ø±Ø§ÛŒ Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø®Ø· ÙØ±Ù…Ø§Ù†
import os  # Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª ÙØ§ÛŒÙ„
import time  # Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ÙØ±ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯
from ultralytics import YOLO  # Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§ÙØ±Ø§Ø¯

# ØªÙ†Ø¸ÛŒÙ… MediaPipe Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø­Ø§Ù„Øª Ø¨Ø¯Ù† (ØªÚ©â€ŒÙ†ÙØ±Ù‡ Ø±ÙˆÛŒ crop)
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.05, model_complexity=2, min_tracking_confidence=0.1)
mp_drawing = mp.solutions.drawing_utils

# Ù…Ø¯Ù„ YOLO Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§ÙØ±Ø§Ø¯
yolo_model = YOLO('yolov8n.pt')  # Ù…Ø¯Ù„ Ø³Ø¨Ú© YOLOv8

# ØªØ§Ø¨Ø¹ Ø§ØµÙ„Ø§Ø­ Ú†Ø±Ø®Ø´ ÙˆÛŒØ¯Ø¦Ùˆ
def rotate_frame(frame, rotation_code=cv2.ROTATE_90_COUNTERCLOCKWISE):
    if rotation_code is None:
        return frame
    return cv2.rotate(frame, rotation_code)

# ØªØ§Ø¨Ø¹ ØªØ´Ø®ÛŒØµ Ø­Ø§Ù„Øª Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙØ±Ø¯
def detect_state(landmarks, prev_landmarks, frame_height):
    state = "Away"
    movement_detected = False
    
    if landmarks:
        hip_y = landmarks[mp_pose.PoseLandmark.LEFT_HIP].y
        shoulder_y = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].y
        if landmarks[mp_pose.PoseLandmark.LEFT_HIP].visibility > 0.2 and \
           landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].visibility > 0.2:
            # ØªØ´Ø®ÛŒØµ Ù†Ø´Ø³ØªÙ† ÛŒØ§ Ø§ÛŒØ³ØªØ§Ø¯Ù†
            if hip_y > 0.4 * frame_height:  # Ú©Ø§Ù‡Ø´ Ø¢Ø³ØªØ§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ ÙˆÛŒØ¯Ø¦ÙˆÙ‡Ø§ÛŒ Ø¹Ù…ÙˆØ¯ÛŒ
                state = "Sitting Idle"
            else:
                state = "Standing Idle"
            
            if prev_landmarks:
                dist = np.linalg.norm(np.array([landmarks[mp_pose.PoseLandmark.NOSE].x,
                                               landmarks[mp_pose.PoseLandmark.NOSE].y]) -
                                     np.array([prev_landmarks[mp_pose.PoseLandmark.NOSE].x,
                                               prev_landmarks[mp_pose.PoseLandmark.NOSE].y]))
                if dist > 0.02 and state == "Standing Idle":  # Ú©Ø§Ù‡Ø´ Ø¢Ø³ØªØ§Ù†Ù‡ Ø­Ø±Ú©Øª
                    state = "Standing Moving"
                elif dist > 0.02:
                    state = "Productive"
    
    return state

# ØªØ§Ø¨Ø¹ ØªØ­Ù„ÛŒÙ„ ÙˆÛŒØ¯Ø¦Ùˆ (Ú†Ù†Ø¯Ù†ÙØ±Ù‡ Ø¨Ø§ YOLO + MediaPipe)
def analyze_video(video_path, rotation_code=cv2.ROTATE_90_COUNTERCLOCKWISE):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError("Ø®Ø·Ø§: Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù† ÙØ§ÛŒÙ„ ÙˆÛŒØ¯Ø¦Ùˆ Ø±Ø§ Ø¨Ø§Ø² Ú©Ø±Ø¯")
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    duration = total_frames / fps
    
    states_history = [[] for _ in range(4)]  # Ø­Ø¯Ø§Ú©Ø«Ø± 4 Ù†ÙØ±
    prev_landmarks_list = [None for _ in range(4)]
    frame_count = 0
    debug_frames = []
    detection_log = []
    
    output_dir = "output"
    frames_dir = os.path.join(output_dir, "frames")
    os.makedirs(frames_dir, exist_ok=True)
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        frame = rotate_frame(frame, rotation_code)
        frame_count += 1
        if frame_count % 3 != 0:
            continue
        
        # ØªØ´Ø®ÛŒØµ Ø§ÙØ±Ø§Ø¯ Ø¨Ø§ YOLO
        yolo_results = yolo_model(frame)
        num_persons = 0
        for result in yolo_results:
            boxes = result.boxes
            for box in boxes:
                if num_persons >= 4:
                    break
                if box.cls == 0:  # class 0 = person
                    x1, y1, x2, y2 = box.xyxy[0].int().tolist()
                    crop = frame[y1:y2, x1:x2]
                    if crop.size == 0:
                        continue
                    crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)
                    pose_results = pose.process(crop_rgb)
                    
                    if pose_results.pose_landmarks:
                        state = detect_state(pose_results.pose_landmarks.landmark, prev_landmarks_list[num_persons], height)
                        states_history[num_persons].append(state)
                        prev_landmarks_list[num_persons] = pose_results.pose_landmarks.landmark
                        mp_drawing.draw_landmarks(crop, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)
                        frame[y1:y2, x1:x2] = crop
                        num_persons += 1
        
        print(f"ÙØ±ÛŒÙ… {frame_count}: {num_persons} Ù†ÙØ± ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯ØŒ Ø­Ø§Ù„Ø§Øª: {[states_history[i][-1] if states_history[i] else 'None' for i in range(4)]}")
        detection_log.append({"frame": frame_count, "num_persons": num_persons, "states": [states_history[i][-1] if states_history[i] else None for i in range(4)]})
        
        if frame_count % 10 == 0:
            cv2.imwrite(os.path.join(frames_dir, f"frame_{frame_count}.jpg"), frame)
            debug_frames.append(frame)
        
        cv2.imshow('Debug Frame', frame)
        cv2.waitKey(1)
    
    cap.release()
    cv2.destroyAllWindows()
    
    pd.DataFrame(detection_log).to_csv(os.path.join(output_dir, "detection_log_v2.csv"), index=False)
    
    reports = []
    for idx, states in enumerate(states_history):
        if not states:
            continue
        state_counts = pd.Series(states).value_counts(normalize=True) * 100
        productive_pct = state_counts.get("Productive", 0) + state_counts.get("Standing Moving", 0)
        unproductive_pct = 100 - productive_pct
        report = {
            "ÙØ±Ø¯": idx + 1,
            "Ù…Ø¯Øª Ø²Ù…Ø§Ù† Ú©Ù„ (Ø«Ø§Ù†ÛŒÙ‡)": duration,
            "Ø±Ø²ÙˆÙ„ÙˆØ´Ù†": f"{width}x{height}",
            "ÙØ±ÛŒÙ… Ø¯Ø± Ø«Ø§Ù†ÛŒÙ‡": fps,
            "ØªØ¹Ø¯Ø§Ø¯ ÙØ±ÛŒÙ…â€ŒÙ‡Ø§": total_frames,
            "Ø¯Ø±ØµØ¯ Ù…ÙˆÙ„Ø¯": productive_pct,
            "Ø¯Ø±ØµØ¯ ØºÛŒØ±Ù…ÙˆÙ„Ø¯": unproductive_pct,
            "ØªÙÚ©ÛŒÚ© Ø­Ø§Ù„Ø§Øª": state_counts.to_dict()
        }
        reports.append(report)
    
    pd.DataFrame(reports).to_csv(os.path.join(output_dir, "analysis_report_v2.csv"), index=False)
    return reports, debug_frames

# ØªØ§Ø¨Ø¹ Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯
def show_dashboard(reports=None, debug_frames=None):
    st.title("ğŸ“Š Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ (Ú†Ù†Ø¯Ù†ÙØ±Ù‡ØŒ Ú†Ù†Ø¯ Ø­Ø§Ù„ØªÙ‡)")
    
    if reports is None:
        st.header("Ø¢Ù¾Ù„ÙˆØ¯ ÙˆÛŒØ¯Ø¦Ùˆ")
        video_file = st.file_uploader("ÙˆÛŒØ¯Ø¦Ùˆ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯ (MP4)", type=["mp4"])
        rotation_option = st.selectbox("Ø¬Ù‡Øª Ú†Ø±Ø®Ø´ ÙˆÛŒØ¯Ø¦Ùˆ", 
                                      ["Ù¾Ø§Ø¯Ø³Ø§Ø¹ØªÚ¯Ø±Ø¯ 90 Ø¯Ø±Ø¬Ù‡", "Ø³Ø§Ø¹ØªÚ¯Ø±Ø¯ 90 Ø¯Ø±Ø¬Ù‡", "180 Ø¯Ø±Ø¬Ù‡", "Ø¨Ø¯ÙˆÙ† Ú†Ø±Ø®Ø´"],
                                      index=0)
        rotation_map = {
            "Ù¾Ø§Ø¯Ø³Ø§Ø¹ØªÚ¯Ø±Ø¯ 90 Ø¯Ø±Ø¬Ù‡": cv2.ROTATE_90_COUNTERCLOCKWISE,
            "Ø³Ø§Ø¹ØªÚ¯Ø±Ø¯ 90 Ø¯Ø±Ø¬Ù‡": cv2.ROTATE_90_CLOCKWISE,
            "180 Ø¯Ø±Ø¬Ù‡": cv2.ROTATE_180,
            "Ø¨Ø¯ÙˆÙ† Ú†Ø±Ø®Ø´": None
        }
        rotation_code = rotation_map[rotation_option]
        
        if video_file:
            video_path = os.path.join("videos", "uploaded_video.mp4")
            os.makedirs("videos", exist_ok=True)
            with open(video_path, "wb") as f:
                f.write(video_file.getbuffer())
            st.success("ÙˆÛŒØ¯Ø¦Ùˆ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯. ØªØ­Ù„ÛŒÙ„ Ø¯Ø± Ø­Ø§Ù„ Ø§Ù†Ø¬Ø§Ù…...")
            reports, debug_frames = analyze_video(video_path, rotation_code)
    
    if reports:
        st.header(f"ØªØ¹Ø¯Ø§Ø¯ Ø§ÙØ±Ø§Ø¯ ØªØ´Ø®ÛŒØµâ€ŒØ¯Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡: {len(reports)}")
        for report in reports:
            st.subheader(f"ØªØ­Ù„ÛŒÙ„ Ø¨Ø±Ø§ÛŒ ÙØ±Ø¯ {report['ÙØ±Ø¯']}")
            st.write(f"Ù…Ø¯Øª Ø²Ù…Ø§Ù†: {report['Ù…Ø¯Øª Ø²Ù…Ø§Ù† Ú©Ù„ (Ø«Ø§Ù†ÛŒÙ‡)']} Ø«Ø§Ù†ÛŒÙ‡")
            st.write(f"Ø±Ø²ÙˆÙ„ÙˆØ´Ù†: {report['Ø±Ø²ÙˆÙ„ÙˆØ´Ù†']}")
            st.write(f"ÙØ±ÛŒÙ… Ø¯Ø± Ø«Ø§Ù†ÛŒÙ‡: {report['ÙØ±ÛŒÙ… Ø¯Ø± Ø«Ø§Ù†ÛŒÙ‡']}")
            st.write(f"ØªØ¹Ø¯Ø§Ø¯ ÙØ±ÛŒÙ…â€ŒÙ‡Ø§: {report['ØªØ¹Ø¯Ø§Ø¯ ÙØ±ÛŒÙ…â€ŒÙ‡Ø§']}")
            
            fig = px.pie(values=[report["Ø¯Ø±ØµØ¯ Ù…ÙˆÙ„Ø¯"], report["Ø¯Ø±ØµØ¯ ØºÛŒØ±Ù…ÙˆÙ„Ø¯"]],
                         names=["Ù…ÙˆÙ„Ø¯", "ØºÛŒØ±Ù…ÙˆÙ„Ø¯"],
                         title=f"ØªÙÚ©ÛŒÚ© Ø¨Ù‡Ø±Ù‡â€ŒÙˆØ±ÛŒ ÙØ±Ø¯ {report['ÙØ±Ø¯']}",
                         color_discrete_sequence=["#00CC96", "#EF553B"])
            st.plotly_chart(fig)
            
            state_counts = pd.Series(report["ØªÙÚ©ÛŒÚ© Ø­Ø§Ù„Ø§Øª"])
            fig_states = px.bar(x=state_counts.index, y=state_counts.values,
                                labels={"x": "Ø­Ø§Ù„Øª", "y": "Ø¯Ø±ØµØ¯ (%)"},
                                title=f"ØªÙˆØ²ÛŒØ¹ Ø­Ø§Ù„Ø§Øª ÙØ±Ø¯ {report['ÙØ±Ø¯']}",
                                color=state_counts.index,
                                color_discrete_map={
                                    "Productive": "#00CC96",
                                    "Sitting Idle": "#EF553B",
                                    "Standing Idle": "#636EFA",
                                    "Standing Moving": "#00CC96",
                                    "Away": "#AB63FA"
                                })
            st.plotly_chart(fig_states)
            
            st.header("Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„")
            st.table(pd.DataFrame([report]))
    
    if debug_frames:
        st.header("ÙØ±ÛŒÙ… Ù†Ù…ÙˆÙ†Ù‡ (Ø¯ÛŒØ¨Ø§Ú¯)")
        frame = debug_frames[0]
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        st.image(frame_rgb, caption="ÙØ±ÛŒÙ… Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø§ Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ", use_column_width=True)
    
    st.header("Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´")
    if reports and st.button("Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø¨Ù‡ JSON"):
        pd.DataFrame(reports).to_json(os.path.join("output", "analysis_report_v2.json"), orient="records", lines=True, force_ascii=False)
        st.success("Ú¯Ø²Ø§Ø±Ø´ Ø¨Ù‡ JSON Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    st.header("Ù„Ø§Ú¯ ØªØ´Ø®ÛŒØµ")
    if os.path.exists(os.path.join("output", "detection_log_v2.csv")):
        detection_log = pd.read_csv(os.path.join("output", "detection_log_v2.csv"))
        st.write("Ù„Ø§Ú¯ ØªØ¹Ø¯Ø§Ø¯ Ø§ÙØ±Ø§Ø¯ Ùˆ Ø­Ø§Ù„Ø§Øª Ø¯Ø± Ù‡Ø± ÙØ±ÛŒÙ…:")
        st.dataframe(detection_log)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ (Ú†Ù†Ø¯Ù†ÙØ±Ù‡ØŒ Ú†Ù†Ø¯ Ø­Ø§Ù„ØªÙ‡)")
    parser.add_argument("--video", type=str, help="Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ ÙˆÛŒØ¯Ø¦Ùˆ")
    parser.add_argument("--dashboard", action="store_true", help="Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ù¾Ø³ Ø§Ø² ØªØ­Ù„ÛŒÙ„")
    args = parser.parse_args()
    
    if args.video:
        reports, debug_frames = analyze_video(args.video)
        print("ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ø´Ø¯. Ú¯Ø²Ø§Ø±Ø´ Ø¯Ø± output/analysis_report_v2.csv Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        print(reports)
        if args.dashboard:
            show_dashboard(reports, debug_frames)
    else:
        show_dashboard()